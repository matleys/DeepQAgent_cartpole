{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6130660",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfc57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from statistics import mean, median\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf68136",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bce2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space # position of cart, velocity of cart, angle of pole, rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db29097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space # move left, move right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dd017",
   "metadata": {},
   "source": [
    "# Random Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a187142",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 score: 40.0\n",
      "Episode 2 score: 10.0\n",
      "Episode 3 score: 19.0\n",
      "Episode 4 score: 28.0\n",
      "Episode 5 score: 13.0\n",
      "Episode 6 score: 8.0\n",
      "Episode 7 score: 28.0\n",
      "Episode 8 score: 24.0\n",
      "Episode 9 score: 13.0\n",
      "Episode 10 score: 25.0\n",
      "Episode 11 score: 18.0\n",
      "Episode 12 score: 18.0\n",
      "Episode 13 score: 15.0\n",
      "Episode 14 score: 15.0\n",
      "Episode 15 score: 26.0\n",
      "Episode 16 score: 13.0\n",
      "Episode 17 score: 9.0\n",
      "Episode 18 score: 21.0\n",
      "Episode 19 score: 37.0\n",
      "Episode 20 score: 21.0\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "scores = []\n",
    "\n",
    "for episode in range(n):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while done == False:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    scores.append(score)\n",
    "    print('Episode', episode+1, 'score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c734a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward random agent:\t 20.05\n",
      "Median reward random agent:\t 18.5\n",
      "Max reward random agent:\t 40.0\n",
      "Total reward random agent:\t 401.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean reward random agent:\\t', mean(scores))\n",
    "print('Median reward random agent:\\t', median(scores))\n",
    "print('Max reward random agent:\\t', max(scores))\n",
    "print('Total reward random agent:\\t', sum(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975074c",
   "metadata": {},
   "source": [
    "# Deep Q Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc38f50",
   "metadata": {},
   "source": [
    "## Build Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf23528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(state_space, action_space):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(action_space,state_space)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='linear')) #Output: Q value estimate\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96aff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 850\n",
      "Trainable params: 850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_nn(state_space, action_space)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f06c5b",
   "metadata": {},
   "source": [
    "## Create RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201b414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(model, action_space):\n",
    "    policy = EpsGreedyQPolicy(eps=0.1) # Epsilon-Greedy policy for exploration vs exploitation trade-off\n",
    "    memory = SequentialMemory(limit=5000, window_length=2)\n",
    "    network = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=action_space, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d00f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 1.0000\n",
      "56 episodes - episode_reward: 177.107 [16.000, 500.000] - loss: 6.852 - mae: 50.975 - mean_q: 102.040\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.0000\n",
      "27 episodes - episode_reward: 358.852 [180.000, 500.000] - loss: 6.182 - mae: 51.221 - mean_q: 102.826\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 1.0000\n",
      "done, took 158.758 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22fbad752b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = build_network(model, action_space)\n",
    "network.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "network.fit(env, nb_steps=30000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2145aa55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n",
      "Episode 11: reward: 500.000, steps: 500\n",
      "Episode 12: reward: 500.000, steps: 500\n",
      "Episode 13: reward: 500.000, steps: 500\n",
      "Episode 14: reward: 500.000, steps: 500\n",
      "Episode 15: reward: 500.000, steps: 500\n",
      "Episode 16: reward: 500.000, steps: 500\n",
      "Episode 17: reward: 500.000, steps: 500\n",
      "Episode 18: reward: 500.000, steps: 500\n",
      "Episode 19: reward: 500.000, steps: 500\n",
      "Episode 20: reward: 500.000, steps: 500\n"
     ]
    }
   ],
   "source": [
    "dqn_perf = network.test(env, nb_episodes=20, visualize=False) # DQN Agent gets the maximum possible score every time\n",
    "dqn_scores = dqn_perf.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d03720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward random agent:\t 20.05\n",
      "Median reward random agent:\t 18.5\n",
      "Max reward random agent:\t 40.0\n",
      "Total reward random agent:\t 401.0\n",
      "----------------------------------------\n",
      "Mean reward DQN agent:\t\t 500.0\n",
      "Median reward DQN agent:\t 500.0\n",
      "Max reward DQN agent:\t\t 500.0\n",
      "Total reward DQN agent:\t\t 10000.0\n"
     ]
    }
   ],
   "source": [
    "# Random Agent performance for 20 episodes\n",
    "print('Mean reward random agent:\\t', mean(scores))\n",
    "print('Median reward random agent:\\t', median(scores))\n",
    "print('Max reward random agent:\\t', max(scores))\n",
    "print('Total reward random agent:\\t', sum(scores))\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "# DQN Agent performance for 20 episodes\n",
    "print('Mean reward DQN agent:\\t\\t', np.mean(dqn_scores))\n",
    "print('Median reward DQN agent:\\t', np.median(dqn_scores))\n",
    "print('Max reward DQN agent:\\t\\t', max(dqn_scores))\n",
    "print('Total reward DQN agent:\\t\\t', sum(dqn_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b659b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n"
     ]
    }
   ],
   "source": [
    "# visual example\n",
    "_ = network.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5266dce",
   "metadata": {},
   "source": [
    "# Save Trained DQN Agent Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc76716",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
